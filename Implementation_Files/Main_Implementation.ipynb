{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0f07d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from torchvision.datasets import MNIST\n",
    "import matplotlib.pyplot as plt\n",
    "from torcheval import metrics\n",
    "\n",
    "\n",
    "import pickle\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "from model_definitions import *\n",
    "from helper_class_definition import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "49c1221f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = MNIST(root = \"data\", download=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a5c5c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n=-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17ce588b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n+=1\n",
    "print(ds.targets[n])\n",
    "plt.imshow(ds.data[n].to(torch.float), cmap=\"grey\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "af129df3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 28, 28])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shaped_data = ds.data.reshape([60000,28,28])\n",
    "shaped_data = shaped_data.to(torch.float)\n",
    "shaped_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc952aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = {\"train\":None, \"test\":None, \"valid\":None}\n",
    "x = {\"train\":None, \"test\":None, \"valid\":None}\n",
    "\n",
    "x[\"train\"], x[\"test\"], y[\"train\"], y[\"test\"] = train_test_split(shaped_data, ds.targets, train_size=0.8)\n",
    "x[\"test\"], x[\"valid\"], y[\"test\"], y[\"valid\"] = train_test_split(x[\"test\"], y[\"test\"], train_size=0.75)\n",
    "\n",
    "x[\"train\"] = x[\"train\"][:20000]\n",
    "y[\"train\"] = y[\"train\"][:20000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4f6c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_dataset = {\"x\":x, \"y\":y}\n",
    "\n",
    "with open(\"C://Austin//U_Cinci//Sem3//DeepLearning//Austin_Paulraj_Homework2//Implementation_Files//processed_dataset.pickle\", 'wb') as handle:\n",
    "    pickle.dump(total_dataset, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1eac7de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('C://Austin//U_Cinci//Sem3//DeepLearning//Austin_Paulraj_Homework2//Implementation_Files//processed_dataset.pickle', 'rb') as handle:\n",
    "    total_dataset = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fee2379",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "new(): invalid data type 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mset\u001b[39m \u001b[38;5;129;01min\u001b[39;00m total_dataset[key]:\n\u001b[1;32m----> 7\u001b[0m       total_dataset[key] \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39minterpolate(\u001b[43mtotal_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mset\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m), size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m224\u001b[39m))\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m#valid_dataset[key] = F.interpolate(valid_dataset[key].unsqueeze(0), size=(224))\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: new(): invalid data type 'str'"
     ]
    }
   ],
   "source": [
    "total_dataset = {\"x\":x[\"train\"], \"y\":y[\"train\"]}\n",
    "valid_dataset = {\"x\":x[\"valid\"], \"y\":y[\"valid\"]}\n",
    "\n",
    "for key in total_dataset.keys():\n",
    "  if key == \"x\":\n",
    "      total_dataset[key] = F.interpolate(total_dataset[key].unsqueeze(0), size=(224))\n",
    "      valid_dataset[key] = F.interpolate(valid_dataset[key].unsqueeze(0), size=(224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "34756490",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepping DS for Resnet, VGG\n",
    "for key in total_dataset.keys():\n",
    "    if key == \"x\":\n",
    "        total_dataset[key] = total_dataset[key].reshape(total_dataset[\"x\"].shape[0],1,224,224)\n",
    "        #valid_dataset[key] = valid_dataset[key].reshape(valid_dataset[key].shape[1],1,224,224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "401f7dc6",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m total_dataset\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m----> 2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain: \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mtotal_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m)\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalid: \u001b[39m\u001b[38;5;124m\"\u001b[39m, valid_dataset[key]\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'dict' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "for key in total_dataset.keys():\n",
    "    print(\"train: \", total_dataset[key].shape)\n",
    "    print(\"valid: \", valid_dataset[key].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "530cc819",
   "metadata": {},
   "outputs": [],
   "source": [
    "#total_dataset = {\"x\":x[\"train\"], \"y\":y[\"train\"]}\n",
    "\n",
    "with open(\"C://Austin//U_Cinci//Sem3//DeepLearning//Austin_Paulraj_Homework2//Implementation_Files//transformed_train_dataset.pickle\", 'wb') as handle:\n",
    "    pickle.dump(total_dataset, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "with open(\"C://Austin//U_Cinci//Sem3//DeepLearning//Austin_Paulraj_Homework2//Implementation_Files//transformed_valid_dataset.pickle\", 'wb') as handle:\n",
    "    pickle.dump(valid_dataset, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d6a0f571",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_definitions import *\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from torcheval.metrics import R2Score\n",
    "\n",
    "class DL_Homework2_helper_class():\n",
    "    \"\"\"\n",
    "    This class is a all in one Trainer class made to efficiently train all the different architectures described at one.\n",
    "    It stors the dataset and has multiple dicts to concurently track the scores, weight updates and individual best models\n",
    "    These dicts are saved in specific folders to be used in the final evaluation of the models\n",
    "\n",
    "    The main functions implemented are:\n",
    "        - validation_hyperparam_test: Uses the validation dataset to quickly test a matrix of hyper parameter combinations\n",
    "        -- The output dictionary of this function Contains the validation R2 scores of every combination tested, and is used to find the\n",
    "        -- hyperparameters that result in each architecture's best model\n",
    "\n",
    "        - train_all_model_types: Trains all the models at once, used for both validation and the actual training steps\n",
    "        -- Saves the best model by comparing R2 at each epoch\n",
    "        -- The \"Out_Dict\" stores model information and epoch level scores to be saved and analyzed\n",
    "\n",
    "        -nn_model_train_step: A helper function used to perform the backpropogation step as designed in pytorch\n",
    "    \"\"\"\n",
    "    def __init__(self, x, y, device=\"cpu\"):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.device= device\n",
    "        self.out_dict = None\n",
    "        self.optimizers = None\n",
    "        self.validation_dict = None\n",
    "        self.best = None\n",
    "\n",
    "    def validation_hyperparam_test(self, loss_functions = [nn.MSELoss], optimizers = [torch.optim.SGD], lr_range = [0.0001, 0.001, 0.01, 0.1], epochs = 100, batch_size=200):\n",
    "        \n",
    "        # main tracking output of the validation step\n",
    "        self.validation_dict = {}\n",
    "        self.out_dict = None\n",
    "\n",
    "        for loss_function in loss_functions:\n",
    "            for optimizer in optimizers:\n",
    "                for lr in lr_range:\n",
    "\n",
    "                    # Loop through all combinations of hyper parameters\n",
    "                    self.train_all_model_types(loss_func=loss_function, optimizer=optimizer, lr=lr, epochs=epochs, batch_size=batch_size, validation=True)\n",
    "                    self.validation_dict[loss_function._get_name()+\"_\"+optimizer.__name__+\"_\"+str(lr)] = self.out_dict[\"Scores\"]\n",
    "\n",
    "                    #reset state dict for next training interation\n",
    "                    self.out_dict = None\n",
    "\n",
    "\n",
    "    def train_all_model_types(self, save_path=\"\",  loss_func=nn.CrossEntropyLoss(), optimizer = torch.optim.Adam, lr=1e-1, epochs=10, train_lr=True, batch_size = 373, validation = False):\n",
    "        # Define out dictionary if it has not been defined before\n",
    "        ## If it has been defined before then it will skip this step and continue training on the same models \n",
    "        if self.out_dict == None:\n",
    "            self.out_dict = {}\n",
    "\n",
    "            # Set up scores dict to track and save Loss/R2 for future analysis\n",
    "            self.out_dict[\"Scores\"] = {}\n",
    "\n",
    "            # Here we set up all the models defined in the \"model_definitions\" module for the different architectures to be tested\n",
    "            self.out_dict[\"DNNNet\"] = DNNNet().to(self.device)\n",
    "            self.out_dict[\"ConvNet\"] = ConvNet().to(self.device) \n",
    "            \n",
    "            self.best = {}\n",
    "\n",
    "            for key in self.out_dict.keys(): \n",
    "                # Here we define what is needed to  be track in the \"Scores\" dict\n",
    "                ## If validation, then we only need the R2 Validation score\n",
    "                ## Else we need R2 and Loss scores on both the Training and Testing models\n",
    "                ### These are used for forming the epoch graphs and tables for model analysis\n",
    "                if validation == False:\n",
    "                    if key != \"Scores\":\n",
    "                        self.out_dict[\"Scores\"][key] = {\"train\":[], \"valid\":[], \"loss\":{\"train\":[], \"valid\":[]}}\n",
    "                        self.best[key] = [0, -1]\n",
    "                else:\n",
    "                    if key != \"Scores\":\n",
    "                        self.out_dict[\"Scores\"][key] = {\"valid\":[]}\n",
    "            \n",
    "            #Pytorch requires separatly defined optimizers based on the model parameters\n",
    "            # Here we set up the model specific optimizers for the training loop\n",
    "            self.optimizers = {\n",
    "                            key : optimizer(self.out_dict[key].parameters(), lr=lr) \n",
    "                                for key in self.out_dict.keys() \n",
    "                                if key!=\"Scores\"\n",
    "                        }\n",
    "        \n",
    "        #TQDM is used to show the progress of the model training by tracking the number of epochs\n",
    "        for epoch in tqdm(range(epochs)):\n",
    "            #print(\"Epoch: \" + str(epoch))\n",
    "            # We take a sub batch of the total train dataset to speed up training iterations\n",
    "            ## With a high enough number of epochs, using this method of batching seems to help model generatlisation when compared to using all of it \n",
    "            for i in range(int(self.x[\"train\"].shape[0]/batch_size)-1):\n",
    "                ind = np.array([k for k in range(self.x[\"train\"].shape[0])])\n",
    "                np.random.shuffle(ind)\n",
    "                if (i+1)*batch_size <= self.x[\"train\"].shape[0]:\n",
    "                    x_batch = torch.Tensor(self.x[\"train\"][i*batch_size : (i+1)*batch_size])\n",
    "                    y_batch = torch.Tensor(self.y[\"train\"][i*batch_size : (i+1)*batch_size])\n",
    "                else:\n",
    "                    x_batch = torch.Tensor(self.x[\"train\"][i*batch_size : -1])\n",
    "                    y_batch = torch.Tensor(self.y[\"train\"][i*batch_size : -1])\n",
    "                \n",
    "                x_batch = x_batch.reshape([x_batch.shape[0], 1, 28 ,28])\n",
    "\n",
    "                for key in self.out_dict.keys(): \n",
    "                    if key != \"Scores\":\n",
    "                        if key == \"DNNNet\":\n",
    "                            x_batch = x_batch.reshape([x_batch.shape[0], 784])\n",
    "\n",
    "                        self.out_dict[key].train(True)\n",
    "                        #Sequentially train each model architecture based on the current feature sampe and target y value\n",
    "                        self.nn_model_train_step(self.out_dict[key], key, self.optimizers[key], x_batch, y_batch, loss_func=loss_func, lr=lr)\n",
    "                        if key == \"DNNNet\":\n",
    "\n",
    "                            x_batch = x_batch.reshape([x_batch.shape[0], 1, 28 ,28])\n",
    "            \n",
    "            out_str = \"\"\n",
    "            for key in self.out_dict.keys(): \n",
    "                if key != \"Scores\":\n",
    "                    self.out_dict[key].eval()\n",
    "                    with torch.no_grad():\n",
    "                    # After each training Epoch, we update the Scores dict with what is required as defined before\n",
    "                        if validation == False:\n",
    "                            \n",
    "                            if key == \"DNNNet\":\n",
    "                                train = self.out_dict[key](self.x[\"train\"].reshape([self.x[\"train\"].shape[0], 784]))\n",
    "                                test = self.out_dict[key](self.x[\"valid\"].reshape([self.x[\"valid\"].shape[0], 784]))\n",
    "                            \n",
    "                            else:\n",
    "                                train = self.out_dict[key](self.x[\"train\"].reshape([self.x[\"train\"].shape[0], 1, 28, 28]))\n",
    "                                test = self.out_dict[key](self.x[\"valid\"].reshape([self.x[\"valid\"].shape[0], 1, 28 ,28]))\n",
    "\n",
    "                            #train = train.reshape([train.shape[0]])\n",
    "                            acc = loss_func(train, self.y[\"train\"])\n",
    "                            self.out_dict[\"Scores\"][key][\"train\"].append(acc)\n",
    "\n",
    "                            #Current epoch test set R2\n",
    "                            \n",
    "                            #test = test.reshape([test.shape[0]])\n",
    "                            acc2 = loss_func(test, torch.Tensor(self.y[\"valid\"]))\n",
    "\n",
    "                            f1 = metrics.MulticlassF1Score()\n",
    "                            f1.update(test, self.y[\"valid\"])\n",
    "                            f1_score = f1.compute()\n",
    "                            self.out_dict[\"Scores\"][key][\"valid\"].append(f1_score)\n",
    "\n",
    "                            if self.best[key][1] < self.out_dict[\"Scores\"][key][\"valid\"][-1]:\n",
    "                                self.best[key][1] = self.out_dict[\"Scores\"][key][\"valid\"][-1]\n",
    "                                self.best[key][0] = epoch\n",
    "                                torch.save(self.out_dict[key], save_path+\"//\"+key+\".pickle\")\n",
    "                            \n",
    "                            #print(str(key)+\": \"+str(self.out_dict[\"Scores\"][key][\"train\"][-1])+ \" | val_f1: \"+str(self.out_dict[\"Scores\"][key][\"valid\"][-1]))\n",
    "                        \n",
    "                        else:\n",
    "                            \n",
    "                            if key == \"DNNNet\":\n",
    "                                train = self.out_dict[key](self.x[\"valid\"].reshape([self.x[\"valid\"].shape[0], 784]))\n",
    "                            \n",
    "                            else:\n",
    "                                train = self.out_dict[key](self.x[\"valid\"].reshape([self.x[\"valid\"].shape[0], 1, 28, 28]))\n",
    "\n",
    "                            #Current epoch validation set R2\n",
    "                            acc = loss_func(train, self.y[\"valid\"])\n",
    "                            self.out_dict[\"Scores\"][key][\"valid\"].append(acc)\n",
    "                        \n",
    "                    \n",
    "\n",
    "    def nn_model_train_step(self, model, key, optimizer, x, y, loss_func=nn.CrossEntropyLoss(), lr=1e-1):\n",
    "        optimizer.zero_grad()\n",
    "        #Training loop step as defined for Pytorch models\n",
    "        pred = model.forward(x)\n",
    "        output = loss_func(pred, y)\n",
    "        #print(output)\n",
    "        output.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "15b6ceff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(1, 8, 2, 2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2,1),\n",
    "            nn.Conv2d(8, 16, 2, 2, 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(6*6*16,288),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(288,10),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.conv_layers(x)\n",
    "    \n",
    "class DNNNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Linear(784, 392),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(392, 196),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(196,10),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.conv_layers(x)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a1ecb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = total_dataset[\"x\"]\n",
    "y = total_dataset[\"y\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "542f6798",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([28, 28])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[\"train\"][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8aa97e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = DL_Homework2_helper_class(x,y,\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "24a81919",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:24<00:00,  1.70s/it]\n",
      "100%|██████████| 50/50 [01:26<00:00,  1.74s/it]\n",
      "100%|██████████| 50/50 [01:51<00:00,  2.22s/it]\n",
      "100%|██████████| 50/50 [02:11<00:00,  2.62s/it]\n",
      "100%|██████████| 50/50 [01:46<00:00,  2.12s/it]\n",
      "100%|██████████| 50/50 [01:45<00:00,  2.10s/it]\n",
      "100%|██████████| 50/50 [01:39<00:00,  1.99s/it]\n",
      "100%|██████████| 50/50 [01:58<00:00,  2.37s/it]\n",
      "100%|██████████| 50/50 [02:02<00:00,  2.45s/it]\n",
      "100%|██████████| 50/50 [01:46<00:00,  2.12s/it]\n",
      "100%|██████████| 50/50 [01:43<00:00,  2.08s/it]\n",
      "100%|██████████| 50/50 [01:30<00:00,  1.81s/it]\n",
      "100%|██████████| 50/50 [01:53<00:00,  2.27s/it]\n",
      "100%|██████████| 50/50 [02:07<00:00,  2.55s/it]\n",
      "100%|██████████| 50/50 [01:41<00:00,  2.03s/it]\n",
      "100%|██████████| 50/50 [01:46<00:00,  2.13s/it]\n",
      "100%|██████████| 50/50 [01:57<00:00,  2.35s/it]\n",
      "100%|██████████| 50/50 [01:55<00:00,  2.31s/it]\n",
      "100%|██████████| 50/50 [01:29<00:00,  1.80s/it]\n",
      "100%|██████████| 50/50 [01:16<00:00,  1.53s/it]\n",
      "100%|██████████| 50/50 [01:21<00:00,  1.63s/it]\n",
      "100%|██████████| 50/50 [01:19<00:00,  1.58s/it]\n",
      "100%|██████████| 50/50 [01:29<00:00,  1.79s/it]\n",
      "100%|██████████| 50/50 [01:24<00:00,  1.68s/it]\n"
     ]
    }
   ],
   "source": [
    "trainer.validation_hyperparam_test(loss_functions=[nn.CrossEntropyLoss()], optimizers=[torch.optim.Adadelta, torch.optim.Adagrad, torch.optim.Adam, torch.optim.SGD], lr_range=[0.1, 0.05, 0.0001, 0.001, 0.005, 0.01], epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7d9f80b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"C://Austin//U_Cinci//Sem3//DeepLearning//Austin_Paulraj_Homework2//Implementation_Files//validation_loss_dict.pickle\", 'wb') as handle:\n",
    "    pickle.dump(trainer.validation_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dcceb4f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['CrossEntropyLoss_Adadelta_0.1', 'CrossEntropyLoss_Adadelta_0.05', 'CrossEntropyLoss_Adadelta_0.0001', 'CrossEntropyLoss_Adadelta_0.001', 'CrossEntropyLoss_Adadelta_0.005', 'CrossEntropyLoss_Adadelta_0.01', 'CrossEntropyLoss_Adagrad_0.1', 'CrossEntropyLoss_Adagrad_0.05', 'CrossEntropyLoss_Adagrad_0.0001', 'CrossEntropyLoss_Adagrad_0.001', 'CrossEntropyLoss_Adagrad_0.005', 'CrossEntropyLoss_Adagrad_0.01', 'CrossEntropyLoss_Adam_0.1', 'CrossEntropyLoss_Adam_0.05', 'CrossEntropyLoss_Adam_0.0001', 'CrossEntropyLoss_Adam_0.001', 'CrossEntropyLoss_Adam_0.005', 'CrossEntropyLoss_Adam_0.01', 'CrossEntropyLoss_SGD_0.1', 'CrossEntropyLoss_SGD_0.05', 'CrossEntropyLoss_SGD_0.0001', 'CrossEntropyLoss_SGD_0.001', 'CrossEntropyLoss_SGD_0.005', 'CrossEntropyLoss_SGD_0.01'])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.validation_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2d07f51c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'DNNNet': [11, 0.13, 'CrossEntropyLoss_Adadelta_0.1'],\n",
       " 'ConvNet': [36, 0.134, 'CrossEntropyLoss_Adam_0.0001']}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Display best hyperparameters \n",
    "best = {\"DNNNet\":[0,np.Inf,\"\"], \"ConvNet\":[0,np.Inf,\"\"]}\n",
    "\n",
    "for key in trainer.validation_dict.keys():\n",
    "    for model in trainer.validation_dict[key]:\n",
    "        max_ind = torch.Tensor(trainer.validation_dict[key][model][\"valid\"]).argmin()\n",
    "        if float(np.round(float(trainer.validation_dict[key][model][\"valid\"][max_ind]), 3)) < best[model][1]:\n",
    "            best[model][0] = int(max_ind)\n",
    "            best[model][1] = float(np.round(float(trainer.validation_dict[key][model][\"valid\"][max_ind]), 3))\n",
    "            best[model][2] = key\n",
    "\n",
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9458fb00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:59<00:00,  2.40s/it]\n"
     ]
    }
   ],
   "source": [
    "trainer = DL_Homework2_helper_class(x,y,\"cpu\")\n",
    "trainer.train_all_model_types(\"C://Austin//U_Cinci//Sem3//DeepLearning//Austin_Paulraj_Homework2//Implementation_Files//saved_models//run1\", epochs=50, batch_size=200, lr=0.1, optimizer=torch.optim.Adadelta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8f4aacec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DNNNet: [23, tensor(0.9703)]\n",
      "ConvNet: [45, tensor(0.9587)]\n"
     ]
    }
   ],
   "source": [
    "for key in trainer.best.keys():\n",
    "    print(key+\": \"+str(trainer.best[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6057d029",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:59<00:00,  2.39s/it]\n"
     ]
    }
   ],
   "source": [
    "trainer2 = DL_Homework2_helper_class(x,y,\"cpu\")\n",
    "trainer2.train_all_model_types(\"C://Austin//U_Cinci//Sem3//DeepLearning//Austin_Paulraj_Homework2//Implementation_Files//saved_models//run2\", epochs=50, batch_size=200, lr=0.0001, optimizer=torch.optim.Adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4851269d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DNNNet: [49, tensor(0.9623)]\n",
      "ConvNet: [47, tensor(0.9613)]\n"
     ]
    }
   ],
   "source": [
    "for key in trainer2.best.keys():\n",
    "    print(key+\": \"+str(trainer2.best[key]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ms_thesis_Env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
